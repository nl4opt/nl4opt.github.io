---
layout: default
title: Leaderboard
permalink: /leaderboard/


---

# Leaderboard

The leaderboard will be updated periodically and each participants submission will be evaluated by our organizers every Wednesday and Friday on the [Pacific Time Zone (PST)](https://time.is/PT). Please refer to <!-- the template in the starter kit and --> our [Submission Page](https://nl4opt.github.io/submissions/) for detailed instructions for the submission. To ensure that your latest submission is properly evaluated, we recommend you to update your submission to the Google Drive folder (provided to you in an email from our organizers) by 8am on the evaluation days (PST). 

This leaderboard was last updated on: August 23rd, 2022

*Baseline results have been updated since revising the `max_length` default value from `100` to `200`*.

## Sub-task 1

| Rank | Team Name                   | Affiliation(s) | F1 Score |
|:----:|:---------------------------:|:--------------:|:--------:|
| 1    | mcmc                        |                | 0.929    |
| 2    | KKKKKi                      | Netease        | 0.912    |
| 3    | Baseline (XLM-RoBERTa-base) | Nl4Opt         | 0.906*   |
| 4    | BK                          |                | 0.891    |
| 5    | VTCC-NLP                    | Viettel        | 0.857    |


*\* Details and a tutorial of the baseline can be found in the [Tutorial page](https://nl4opt.github.io/tutorial/).*

* For this challenge, the **micro-averaged F1 score** is the evaluation metric. This measure is described in detail in the metrics section of our [homepage](https://nl4opt.github.io/). 

## Sub-task 2

| Rank | Team Name       | Affiliation(s) | Accuracy |
|:----:|:---------------:|:--------------:|:--------:|
| 1    | Baseline (BART) | NL4Opt         | 0.61*    |
|      |                 |                |          |
|      |                 |                |          |

*\* Details and a tutorial of the baseline can be found in the [Tutorial page](https://nl4opt.github.io/tutorial/).*

* For this challenge, the **declaration-level mapping accuracy** is the evaluation metric. This measure is described in detail in the metrics section of our [homepage](https://nl4opt.github.io/).
