---
layout: default
title: Leaderboard
permalink: /leaderboard/


---

# Leaderboard

The leaderboard will be updated periodically and each participants submission will be evaluated by our organizers every Wednesday and Friday on the [Pacific Time Zone (PST)](https://time.is/PT). Please refer to <!-- the template in the starter kit and --> our [Submission Page](https://nl4opt.github.io/submissions/) for detailed instructions for the submission. To ensure that your latest submission is properly evaluated, we recommend you to update your submission to the Google Drive folder (provided to you in an email from our organizers) by 8am on the evaluation days (PST). 

This leaderboard was last updated on: October 16th, 2022

## Sub-task 1

| Rank | Team Name                   | Affiliation(s)                   | F1 Score | Team Members |
|:----:|:---------------------------:|:--------------------------------:|:--------:|:------------:|
| 1    | __Infrrd AI Lab__           |  __Infrrd__                      | __0.939__|              |
| 2    | __mcmc__                    |  __OPD__                         | __0.933__|              |
| 3    | __PingAn-zhiniao__          |  __PingAn Technology__           | __0.932__|              |
| 4    | __Long__                    |  __BDAA-BASE__                   | __0.931__|              |
| 5    | __VTCC-NLP__                |  __Viettel__                     | __0.929__|              |
| 6    | Sjang                       |  POSTECH                     | 0.927    |              |
| 7    | DeepBlueAI                  |  DeepBlueAI                  | 0.921    |              |
| 8    | TeamFid                     |  Fidelity                    | 0.920    |              |
| 9    | KKKKKi                      |  Netease                     | 0.917    |              |
| 10   | holajoa                    |  Imperial College London      | 0.910    |              |
| 11   | Dream                      |                                  | 0.884    |              |
| -   | macd                        |                                  | 0.987    |              |
| -   | rs                          |                                  | 0.982    |              |
| -   | LeNam                       | VNUHCM                           | 0.926    |              |
| -   | Try1try                     | GWU                              | 0.924    |              |
| -   | BK                          |                                  | 0.918    |              |
| -   | UIUC-NLP                    | UIUC                             | 0.912    |              |
| -   | Baseline (XLM-RoBERTa-base) | Nl4Opt                           | 0.906*   |              |
| -   | CTRI_ysy                    | China Telecom Research Institute | 0.902    |              |
| -   | CUFE                        | Cairo University                 | 0.889    |              |

__Bold__ names indicate winning teams (tentative), only reproduced submissions are ranked.
*\* Details and a tutorial of the baseline can be found in the [Tutorial page](https://nl4opt.github.io/tutorial/).*

* For this challenge, the **micro-averaged F1 score** is the evaluation metric. This measure is described in detail in the metrics section of our [homepage](https://nl4opt.github.io/). 

## Sub-task 2

| Rank | Team Name       | Affiliation(s)    | Accuracy | Team Members |
|:----:|:---------------:|:-----------------:|:--------:|:------------:|
| 1    | __UIUC-NLP__        | __UIUC__              | __0.899__    |              |
| 2    | __Sjang__           | __POSTECH__           | __0.878__    |              |
| 3    | __Long__            | __BDAA-BASE__         | __0.867__    |              |
| 4    | __PingAn-zhiniao__  | __PingAn Technology__ | __0.866__    |              |
| 5    | __Infrrd AI Lab__   | __Infrrd__            | __0.780__    |              |
| 6    | KKKKKi          | Netease           | 0.815    |              |
| -   | Baseline (BART) | NL4Opt            | 0.608*   |              |
| -   | Dream           |                   | 0.608    |              |
| -   | CUFE            |                   | 0.608    |              |
| -   | November        | FSU-Jena          | 0.496    |              |

__Bold__ names indicate winning teams (tentative), only reproduced submissions are ranked.
*\* Details and a tutorial of the baseline can be found in the [Tutorial page](https://nl4opt.github.io/tutorial/).*

* For this challenge, the **declaration-level mapping accuracy** is the evaluation metric. This measure is described in detail in the metrics section of our [homepage](https://nl4opt.github.io/).
