---
layout: default
title: Leaderboard
permalink: /leaderboard/


---

# Leaderboard

The leaderboard will be updated periodically and each participants submission will be evaluated by our organizers every Wednesday and Friday on the [Pacific Time Zone (PST)](https://time.is/PT). Please refer to <!-- the template in the starter kit and --> our [Submission Page](https://nl4opt.github.io/submissions/) for detailed instructions for the submission. To ensure that your latest submission is properly evaluated, we recommend you to update your submission to the Google Drive folder (provided to you in an email from our organizers) by 8am on the evaluation days (PST). 

This leaderboard was last updated on: October 12th, 2022

## Sub-task 1

| Rank | Team Name                   | Affiliation(s)                   | F1 Score | Team Members |
|:----:|:---------------------------:|:--------------------------------:|:--------:|:------------:|
| 1    | rs                          |                                  | 0.982    |              |
| 2    | KKKKKi                      | Netease                          | 0.957    |              |
| 3    | HUHU                        |                                  | 0.945    |              |
| 4    | Long                        | BDAA-BASE                        | 0.943    |              |
| 5    | mcmc                        |                                  | 0.942    |              |
| 6    | Infrrd AI Lab               | Infrrd                           | 0.942    |              |
| 7    | Dream                       |                                  | 0.941    |              |
| 8    | PingAn-zhiniao              | PingAn Technology                | 0.935    |              |
| 9    | Sjang                       | POSTECH                          | 0.930    |              |
| 10   | VTCC-NLP                    | Viettel                          | 0.929    |              |
| 11   | TeamFid                     | Fidelity                         | 0.927    |              |
| 12   | LeNam                       | VNUHCM                           | 0.926    |              |
| 13   | Try1try                     | GWU                              | 0.924    |              |
| 14   | DeepBlueAI                  | DeepBlueAI                       | 0.923    |              |
| 15   | BK                          |                                  | 0.918    |              |
| 16   | holajoa                     | Imperial College London          | 0.915    |              |
| 17   | UIUC-NLP                    | UIUC                             | 0.912    |              |
| 18   | Baseline (XLM-RoBERTa-base) | Nl4Opt                           | 0.906*   |              |
| 19   | CTRI_ysy                    | China Telecom Research Institute | 0.902    |              |
| 20   | macd                        |                                  | 0.891    |              |
| 21   | CUFE                        | Cairo University                 | 0.889    |              |

*\* Details and a tutorial of the baseline can be found in the [Tutorial page](https://nl4opt.github.io/tutorial/).*

* For this challenge, the **micro-averaged F1 score** is the evaluation metric. This measure is described in detail in the metrics section of our [homepage](https://nl4opt.github.io/). 

## Sub-task 2

| Rank | Team Name       | Affiliation(s)    | Accuracy | Team Members |
|:----:|:---------------:|:-----------------:|:--------:|:------------:|
| 1    | Sjang           | POSTECH           | 0.892    |              |
| 2    | PingAn-zhiniao  | PingAn Technology | 0.858    |              |
| 3    | UIUC-NLP        | UIUC              | 0.853    |              |
| 4    | Long            |                   | 0.831    |              |
| 5    | KKKKKi          | Netease           | 0.815    |              |
| 6    | Infrrd AI Lab   | Infrrd            | 0.732    |              |
| T7   | Baseline (BART) | NL4Opt            | 0.608*   |              |
| T7   | Dream           |                   | 0.608    |              |
| T7   | CUFE            |                   | 0.608    |              |
| 11   | November        | FSU-Jena          | 0.496    |              |

*\* Details and a tutorial of the baseline can be found in the [Tutorial page](https://nl4opt.github.io/tutorial/).*

* For this challenge, the **declaration-level mapping accuracy** is the evaluation metric. This measure is described in detail in the metrics section of our [homepage](https://nl4opt.github.io/).
